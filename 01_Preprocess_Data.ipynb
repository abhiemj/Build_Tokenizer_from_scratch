{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2XTtnzkk_Yj",
        "outputId": "d644ccb2-9be4-4ae7-acc1-0f5380326add"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF\n",
            "  Downloading pypdf-3.17.0-py3-none-any.whl (277 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.4/277.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF\n",
            "Successfully installed PyPDF-3.17.0\n"
          ]
        }
      ],
      "source": [
        "# ! pip install langchain\n",
        "# !pip install PyPDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"/content/Business_Statistics_Complete_Business_St_aczel.pdf\")\n",
        "pages = loader.load()"
      ],
      "metadata": {
        "id": "bGUNa5KHuuEW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total number of pages {len(pages)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8P9-WKrJwLeD",
        "outputId": "bade88cd-af63-4eb7-bcb8-d375ff0684c9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of pages 888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the content into text format"
      ],
      "metadata": {
        "id": "6-RuToDV3O8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "page1 = pages[1]\n",
        "page1.page_content[:]"
      ],
      "metadata": {
        "id": "qrH3LYhjxccL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## open a new text file\n",
        "with open('Text_file.txt','w'):\n",
        "  ## loop over through every document inside the pdf\n",
        "  for doc in range(len(pages)):\n",
        "    page = pages[doc]\n",
        "    content = page.page_content[:]\n",
        "    ## append the content\n",
        "    with open('Text_file.txt','a') as file:\n",
        "      # write content in the file\n",
        "      file.write(content)\n",
        ""
      ],
      "metadata": {
        "id": "HYYu49S8wvLD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spli text into sentence tokenizer\n",
        "  - Use nltk"
      ],
      "metadata": {
        "id": "yz0PKs2s3YkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install nltk\n",
        "# !pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0Ve-e7yx-vs",
        "outputId": "912ea5d6-4958-40d5-aaa1-440a0e26b42d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import os\n",
        "import nltk\n",
        "import pandas as pd\n",
        "\n",
        "## Download the punkt tokenizer models\n",
        "nltk.download('punkt')\n",
        "\n",
        "## convert the texts into sentence\n",
        "def text_to_sentence(filepath):\n",
        "  ## open the text file\n",
        "  with open(filepath,'r')as file:\n",
        "    # read the file\n",
        "    text = file.read()\n",
        "    # create sentence tokenizer\n",
        "    sentences = nltk.tokenize.sent_tokenize(text)\n",
        "  return sentences\n",
        "\n",
        "# convert sentences to dataframe\n",
        "def append_sentences_to_df(sentences,df):\n",
        "\n",
        "  for sentence in sentences:\n",
        "    df = df.append({'Sentence': sentence},ignore_index = True)\n",
        "  return df\n",
        "\n",
        "## Initialize a empty df\n",
        "df = pd.DataFrame(columns=[\"text\"])\n",
        "\n",
        "## Intiate a empty list to add all sentence\n",
        "all_sentences = []\n",
        "\n",
        "## add files in the list\n",
        "all_sentences.extend(text_to_sentence('/content/Text_file.txt'))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ7b0BO03l9t",
        "outputId": "dbf3343b-b44a-4651-abfc-4ef45a2d002f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_sentences"
      ],
      "metadata": {
        "id": "qaMQ-1cd3xog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## convert these all sentences to dataframe\n",
        "dataframe = pd.DataFrame(all_sentences,columns=[\"Sentence\"])\n",
        "\n",
        "## save the data frame to csv file\n",
        "dataframe.to_csv(\"text_to_sentences.csv\",index = False)\n"
      ],
      "metadata": {
        "id": "Ewq4Z44B6mgW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre process the text"
      ],
      "metadata": {
        "id": "BTOAjJ6f7aNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## define functions to clean the data\n",
        "## Keep full stops , question marks, exclamation marks semi colon ,comma,minverted commas\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "## define a function to convert to AscII\n",
        "def to_ascii(text):\n",
        "  convert_to_ascii = unicodedata.normalize('NFKD',data)\n",
        "  return convert_to_ascii.encode('ascii','ignore').decode('ascii')\n",
        "\n",
        "## define a function to clean the text\n",
        "def clean_the_data(data):\n",
        "  ## convert to lowercase and to Ascii\n",
        "  data = data.lower()\n",
        "  # remove extra characters\n",
        "  data = re.sub(r'[^a-z\\s]+',' ',data)\n",
        "  ## Make extra space to single space\n",
        "  data = re.sub(r'\\s+',' ',data).strip()\n",
        "  return data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q_P4SKrq7YWv"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Clean the data\n",
        "\n",
        "## read the content of file\n",
        "with open('/content/Text_file.txt','r',encoding ='utf-8') as file:\n",
        "  content = file.read()\n",
        "\n",
        "# cleant the text\n",
        "clean_data = clean_the_data(content)\n",
        "\n",
        "## Rewrite the  clean content back to file\n",
        "with open('/content/Text_file.txt','w',encoding = 'utf-8') as file:\n",
        "  file.write(clean_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "-ZGkkugR-zfK"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Unique words to use As Tokenizers"
      ],
      "metadata": {
        "id": "gxhG0niNBCx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## define a function to extract unique words\n",
        "\n",
        "def get_unique_words(text):\n",
        "  ## split text into words on basis od space\n",
        "  words = text.split()\n",
        "  # Get unique words using set\n",
        "  unique_words = set(words)\n",
        "  return unique_words\n",
        "\n",
        "# Read the cleaned text\n",
        "\n",
        "with open('/content/Text_file.txt','r',encoding = 'utf-8') as f:\n",
        "  content = f.read()\n",
        "\n",
        "# Get the unique qords from text\n",
        "set_of_unique_words = get_unique_words(content)\n",
        "## sort alphabetically into list\n",
        "list_of_unique_words = sorted(list(set_of_unique_words))\n",
        "\n",
        "## save the words to a new text file\n",
        "with open('unique_words.txt','w') as f:\n",
        "  for word in list_of_unique_words:\n",
        "    f.write(word + '\\n')"
      ],
      "metadata": {
        "id": "uuXOCbYOAzUU"
      },
      "execution_count": 35,
      "outputs": []
    }
  ]
}